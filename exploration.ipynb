{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import hydra\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = np.random.rand(1000, 300)\n",
    "time_series[500:560, 100:200] += 0.3\n",
    "time_series = torch.from_numpy(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyAttention(nn.Module):\n",
    "    def __init__(self, seq_dim, channels):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.Q = self.K = self.V = self.sigma = torch.zeros((seq_dim, channels))\n",
    "        self.d_model = channels\n",
    "        self.n  = seq_dim\n",
    "        self.P = torch.zeros((seq_dim, seq_dim))\n",
    "        self.S = torch.zeros((seq_dim, seq_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.initialize(x)\n",
    "        self.P = self.prior_association()\n",
    "        self.S = self.series_association()\n",
    "        print(self.S.shape)\n",
    "        # assert self.S.shape == (self.n, self.n)\n",
    "        Z = self.reconstruction()\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def initialize(self, x):\n",
    "        # self.d_model = x.shape[-1]\n",
    "        self.Q = self.K = self.V = self.sigma = x\n",
    "\n",
    "    def prior_association(self):\n",
    "        return torch.ones((self.n, self.n))\n",
    "\n",
    "    def series_association(self):\n",
    "        print(self.Q.shape)\n",
    "        print(self.K.shape)\n",
    "        return F.softmax((self.Q @ self.K.T) / math.sqrt(self.d_model), dim=0)\n",
    "\n",
    "    def reconstruction(self):\n",
    "        return self.S @ self.V\n",
    "\n",
    "    def association_discrepancy(self):\n",
    "        return F.kl_div(self.P, self.S) + F.kl_div(self.S, self.P) #not going to be correct dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, x):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTransformerBlock(nn.Module):\n",
    "    def __init__(self, seq_dim, feat_dim):\n",
    "        super().__init__()\n",
    "        self.seq_dim, self.feat_dim = seq_dim, feat_dim\n",
    "       \n",
    "        self.attention = AnomalyAttention(self.seq_dim, self.feat_dim)\n",
    "        self.ln1 = nn.LayerNorm(self.feat_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, self.feat_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(self.feat_dim)\n",
    "        self.association_discrepancy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_identity = x \n",
    "        x = self.attention(x)\n",
    "        z = self.ln1(x + x_identity)\n",
    "        \n",
    "        z_identity = z\n",
    "        z = self.ff(z)\n",
    "        z = self.ln2(z + z_identity)\n",
    "\n",
    "        self.association_discrepancy = self.attention.association_discrepancy().detach()\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, seqs, in_channels, layers, lambda_):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AnomalyTransformerBlock(seqs, in_channels) for _ in range(layers)\n",
    "        ])\n",
    "        self.output = None\n",
    "        self.lambda_ = lambda_\n",
    "        self.assoc_discrepancy = torch.zeros((seqs, len(self.blocks)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            self.assoc_discrepancy[:, idx] = block.association_discrepancy\n",
    "        \n",
    "        self.assoc_discrepancy = self.assoc_discrepancy.sum(dim=1) #N x 1\n",
    "        self.output = x\n",
    "        return x\n",
    "\n",
    "    def loss(self, x):\n",
    "        l2_norm = torch.linalg.matrix_norm(self.output - x, ord=2)\n",
    "        return l2_norm + (lambda_ * self.assoc_discrepancy)\n",
    "\n",
    "    def anomaly_score(self, x):\n",
    "        score = F.softmax(-self.assoc_discrepancy, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AnomalyTransformer(seqs=1000, in_channels=300, layers=3, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 300])\n",
      "torch.Size([1000, 300])\n",
      "torch.Size([1000, 1000])\n",
      "torch.Size([1000, 300])\n",
      "torch.Size([1000, 300])\n",
      "torch.Size([1000, 1000])\n",
      "torch.Size([1000, 300])\n",
      "torch.Size([1000, 300])\n",
      "torch.Size([1000, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7510,  0.2225,  0.3001,  ...,  0.3264, -0.5485,  0.1157],\n",
       "        [ 1.3590,  0.1651, -0.0513,  ..., -0.5366, -0.9558, -0.7117],\n",
       "        [ 1.1552, -1.4263,  0.4981,  ...,  2.4720,  0.1922,  0.8349],\n",
       "        ...,\n",
       "        [ 0.5864, -0.5258, -1.7364,  ...,  0.6998, -1.2601, -1.3201],\n",
       "        [-0.4978, -0.2980, -0.4134,  ...,  1.4900, -1.1473,  1.9635],\n",
       "        [ 1.1724, -0.7978,  2.7319,  ..., -1.9063, -0.3888, -0.3263]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(time_series.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f28f51a40c57d9bec6a3c7c5ba9f41b1bc9273fe115e19bff919e2ad7386eeca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dl': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
